{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhagihara/Symptom-Prognosis-Prediction/blob/main/bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTni_TiNmMBW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUuap_cbnSdR",
        "outputId": "be22ce49-fd84-463f-8592-6859056dd68d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Symptoms_training.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3800b50209c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Symptoms_training.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Symptoms_testing.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Symptoms_training.csv'"
          ]
        }
      ],
      "source": [
        "training = pd.read_csv(\"Symptoms_training.csv\")\n",
        "testing = pd.read_csv(\"Symptoms_testing.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW_81NxImmpB"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRd2tzN1mgkP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "class LayerDense:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weight = tf.Variable(tf.random.normal([input_size, output_size]), dtype=tf.float32)\n",
        "\n",
        "        self.bias = tf.Variable(tf.random.normal([output_size]), dtype=tf.float32)\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = tf.cast(input, dtype=tf.float32)\n",
        "        self.output = tf.add(tf.matmul(input, self.weight), self.bias)\n",
        "        return self.output\n",
        "\n",
        "class ActivationRelu:\n",
        "    def forward(self, input):\n",
        "        self.output = tf.nn.relu(input)\n",
        "        return self.output\n",
        "\n",
        "class ActivationSoftmax:\n",
        "    def forward(self, input):\n",
        "        exponent = tf.exp(input - tf.reduce_max(input, axis=1, keepdims=True))\n",
        "        self.output = exponent / tf.reduce_sum(exponent, axis=1, keepdims=True)\n",
        "        return self.output\n",
        "\n",
        "class LossCategoricalCrossentropy:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        samples = len(y_pred)\n",
        "        y_pred_clipped = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = tf.cast(y_true, dtype=tf.int32)\n",
        "            y_true_one_hot = tf.one_hot(y_true, depth=y_pred.shape[1])\n",
        "        else:\n",
        "            y_true_one_hot = y_true\n",
        "\n",
        "        correct_confidences = tf.reduce_sum(y_pred_clipped * y_true_one_hot, axis=1)\n",
        "        negative_log_likelihoods = -tf.math.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    def calculate(self, output, y):\n",
        "        sample_losses = self.forward(output, y)\n",
        "        data_loss = tf.reduce_mean(sample_losses)\n",
        "        return data_loss\n",
        "\n",
        "class OptimizerSGD:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update_parameters(self, trainable_variables, gradients):\n",
        "        updated_weights = []\n",
        "        for var, grad in zip(trainable_variables, gradients):\n",
        "            if isinstance(var, tf.Variable):\n",
        "                updated_weights.append(var - self.learning_rate * grad)\n",
        "            else:\n",
        "                updated_weights.append(var)\n",
        "        return updated_weights\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes):\n",
        "        self.layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(LayerDense(layer_sizes[i], layer_sizes[i + 1]))\n",
        "            stddev = np.sqrt(2 / (layer_sizes[i] + layer_sizes[i + 1]))\n",
        "            self.layers[-1].weight.assign(tf.random.normal([layer_sizes[i], layer_sizes[i + 1]], stddev=stddev))\n",
        "        self.activations = [ActivationRelu() for _ in range(len(layer_sizes) - 2)] + [ActivationSoftmax()]\n",
        "        self.loss = LossCategoricalCrossentropy()\n",
        "        self.optimizer = OptimizerSGD(learning_rate=0.1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.activations[0].forward(self.layers[0].forward(x))\n",
        "        for l in range(1, len(self.layers)):\n",
        "            self.activations[l].forward(self.layers[l].forward(self.activations[l - 1].output))\n",
        "        return self.activations[-1].output\n",
        "\n",
        "    def trainable_variables(self):\n",
        "      variables = []\n",
        "      for layer in self.layers:\n",
        "          variables.append(layer.weight)\n",
        "          variables.append(layer.bias)\n",
        "      return variables\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.forward(x)\n",
        "            loss_value = self.loss.calculate(predictions, y)\n",
        "        gradients = tape.gradient(loss_value, self.trainable_variables())\n",
        "\n",
        "\n",
        "        updated_weights = self.optimizer.update_parameters(self.trainable_variables(), gradients)\n",
        "\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            layer.weight.assign(updated_weights[2*i])\n",
        "            layer.bias.assign(updated_weights[2*i + 1])\n",
        "\n",
        "        return loss_value\n",
        "\n",
        "    def compute_accuracy(model, X_test, y_test, label_encoder):\n",
        "      predictions = model.forward(X_test)\n",
        "      y_pred_encoded = tf.argmax(predictions, axis=1)\n",
        "      y_pred = label_encoder.inverse_transform(y_pred_encoded.numpy())\n",
        "\n",
        "      correct_predictions = np.sum(y_pred == y_test)\n",
        "      total_samples = len(y_test)\n",
        "      accuracy = correct_predictions / total_samples\n",
        "\n",
        "      return accuracy\n",
        "\n",
        "\n",
        "y_train = training['prognosis']\n",
        "y_test = testing['prognosis']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "training_copy = training.copy()\n",
        "testing_copy = testing.copy()\n",
        "training_copy.drop(['Unnamed: 0', 'prognosis'], axis=1, inplace=True)\n",
        "testing_copy.drop(['prognosis'], axis=1, inplace=True)\n",
        "\n",
        "input_size = training_copy.shape[1]\n",
        "output_size = len(np.unique(y_train_encoded))\n",
        "\n",
        "model = NeuralNetwork([input_size, 128, 128, 128, 128, 128, 128, output_size])\n",
        "\n",
        "\n",
        "epochs = 10000\n",
        "for epoch in range(epochs):\n",
        "    model.forward  (training_copy)\n",
        "    loss = model.backward(training_copy, y_train_encoded)\n",
        "    accuracy = model.compute_accuracy(testing_copy, y_test, label_encoder)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss}, Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z36uHswj7spY"
      },
      "outputs": [],
      "source": [
        "testing_copy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}