{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhagihara/Symptom-Prognosis-Prediction/blob/main/bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTni_TiNmMBW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUuap_cbnSdR",
        "outputId": "be22ce49-fd84-463f-8592-6859056dd68d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Symptoms_training.csv'",
          "traceback": [
           ""
          ]
        }
      ],
      "source": [
        "training = pd.read_csv(\"Symptoms_training.csv\")\n",
        "testing = pd.read_csv(\"Symptoms_testing.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW_81NxImmpB"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRd2tzN1mgkP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "class LayerDense:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weight = tf.Variable(tf.random.normal([input_size, output_size]), dtype=tf.float32)\n",
        "\n",
        "        self.bias = tf.Variable(tf.random.normal([output_size]), dtype=tf.float32)\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = tf.cast(input, dtype=tf.float32)\n",
        "        self.output = tf.add(tf.matmul(input, self.weight), self.bias)\n",
        "        return self.output\n",
        "\n",
        "class ActivationRelu:\n",
        "    def forward(self, input):\n",
        "        self.output = tf.nn.relu(input)\n",
        "        return self.output\n",
        "\n",
        "class ActivationSoftmax:\n",
        "    def forward(self, input):\n",
        "        exponent = tf.exp(input - tf.reduce_max(input, axis=1, keepdims=True))\n",
        "        self.output = exponent / tf.reduce_sum(exponent, axis=1, keepdims=True)\n",
        "        return self.output\n",
        "\n",
        "class LossCategoricalCrossentropy:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        samples = len(y_pred)\n",
        "        y_pred_clipped = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = tf.cast(y_true, dtype=tf.int32)\n",
        "            y_true_one_hot = tf.one_hot(y_true, depth=y_pred.shape[1])\n",
        "        else:\n",
        "            y_true_one_hot = y_true\n",
        "\n",
        "        correct_confidences = tf.reduce_sum(y_pred_clipped * y_true_one_hot, axis=1)\n",
        "        negative_log_likelihoods = -tf.math.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    def calculate(self, output, y):\n",
        "        sample_losses = self.forward(output, y)\n",
        "        data_loss = tf.reduce_mean(sample_losses)\n",
        "        return data_loss\n",
        "\n",
        "class OptimizerSGD:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def update_parameters(self, trainable_variables, gradients):\n",
        "        updated_weights = []\n",
        "        for var, grad in zip(trainable_variables, gradients):\n",
        "            if isinstance(var, tf.Variable):\n",
        "                updated_weights.append(var - self.learning_rate * grad)\n",
        "            else:\n",
        "                updated_weights.append(var)\n",
        "        return updated_weights\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes):\n",
        "        self.layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(LayerDense(layer_sizes[i], layer_sizes[i + 1]))\n",
        "            stddev = np.sqrt(2 / (layer_sizes[i] + layer_sizes[i + 1]))\n",
        "            self.layers[-1].weight.assign(tf.random.normal([layer_sizes[i], layer_sizes[i + 1]], stddev=stddev))\n",
        "        self.activations = [ActivationRelu() for _ in range(len(layer_sizes) - 2)] + [ActivationSoftmax()]\n",
        "        self.loss = LossCategoricalCrossentropy()\n",
        "        self.optimizer = OptimizerSGD(learning_rate=0.1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.activations[0].forward(self.layers[0].forward(x))\n",
        "        for l in range(1, len(self.layers)):\n",
        "            self.activations[l].forward(self.layers[l].forward(self.activations[l - 1].output))\n",
        "        return self.activations[-1].output\n",
        "\n",
        "    def trainable_variables(self):\n",
        "      variables = []\n",
        "      for layer in self.layers:\n",
        "          variables.append(layer.weight)\n",
        "          variables.append(layer.bias)\n",
        "      return variables\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.forward(x)\n",
        "            loss_value = self.loss.calculate(predictions, y)\n",
        "        gradients = tape.gradient(loss_value, self.trainable_variables())\n",
        "\n",
        "\n",
        "        updated_weights = self.optimizer.update_parameters(self.trainable_variables(), gradients)\n",
        "\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            layer.weight.assign(updated_weights[2*i])\n",
        "            layer.bias.assign(updated_weights[2*i + 1])\n",
        "\n",
        "        return loss_value\n",
        "\n",
        "    def compute_accuracy(model, X_test, y_test, label_encoder):\n",
        "      predictions = model.forward(X_test)\n",
        "      y_pred_encoded = tf.argmax(predictions, axis=1)\n",
        "      y_pred = label_encoder.inverse_transform(y_pred_encoded.numpy())\n",
        "\n",
        "      correct_predictions = np.sum(y_pred == y_test)\n",
        "      total_samples = len(y_test)\n",
        "      accuracy = correct_predictions / total_samples\n",
        "\n",
        "      return accuracy\n",
        "\n",
        "\n",
        "y_train = training['prognosis']\n",
        "y_test = testing['prognosis']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "training_copy = training.copy()\n",
        "testing_copy = testing.copy()\n",
        "training_copy.drop(['Unnamed: 0', 'prognosis'], axis=1, inplace=True)\n",
        "testing_copy.drop(['prognosis'], axis=1, inplace=True)\n",
        "\n",
        "input_size = training_copy.shape[1]\n",
        "output_size = len(np.unique(y_train_encoded))\n",
        "\n",
        "model = NeuralNetwork([input_size, 128, 128, 128, 128, 128, 128, output_size])\n",
        "\n",
        "\n",
        "epochs = 10000\n",
        "for epoch in range(epochs):\n",
        "    model.forward  (training_copy)\n",
        "    loss = model.backward(training_copy, y_train_encoded)\n",
        "    accuracy = model.compute_accuracy(testing_copy, y_test, label_encoder)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss}, Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z36uHswj7spY"
      },
      "outputs": [],
      "source": [
        "testing_copy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
